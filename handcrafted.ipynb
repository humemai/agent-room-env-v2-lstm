{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data and parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.disabled = True\n",
    "\n",
    "from agent import HandcraftedAgent\n",
    "\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from humemai.utils import read_yaml\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "num_seeds = 1\n",
    "for room_size in [\n",
    "    \"xxs\",\n",
    "    \"xs\",\n",
    "    \"s\",\n",
    "    \"m\",\n",
    "    \"l\",\n",
    "    \"xl\",\n",
    "    \"xxl\",\n",
    "]:\n",
    "\n",
    "    env_str = \"room_env:RoomEnv-v2\"\n",
    "    env_config = {\n",
    "        \"question_prob\": 1.0,\n",
    "        \"terminates_at\": 99,\n",
    "        \"randomize_observations\": \"objects\",\n",
    "        \"room_size\": room_size,\n",
    "        \"rewards\": {\"correct\": 1, \"wrong\": 0, \"partial\": 0},\n",
    "        \"make_everything_static\": False,\n",
    "        \"num_total_questions\": 1000,\n",
    "        \"question_interval\": 5,\n",
    "        \"include_walls_in_observations\": True,\n",
    "        \"deterministic_objects\": False,\n",
    "    }\n",
    "\n",
    "    if \"different-prob\" in room_size:\n",
    "        root_path = f\"./training-results/non-equal-object-probs/handcrafted/\"\n",
    "    else:\n",
    "        root_path = f\"./training-results/equal-object-probs/handcrafted/\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for mm_policy in [\"handcrafted\", \"random\", \"episodic\", \"semantic\"]:\n",
    "        for capacity_max in [2, 6, 12, 24, 48, 96, 192]:\n",
    "            if mm_policy == \"random\" or mm_policy == \"handcrafted\":\n",
    "                qa_function_ = [\n",
    "                    \"episodic_semantic\",\n",
    "                    \"episodic\",\n",
    "                    \"semantic\",\n",
    "                    \"random\",\n",
    "                ]\n",
    "                pretrain_semantic_ = [False, \"include_walls\", \"exclude_walls\"]\n",
    "                capacity = {\n",
    "                    \"episodic\": capacity_max // 2,\n",
    "                    \"semantic\": capacity_max // 2,\n",
    "                    \"short\": 1,\n",
    "                }\n",
    "                semantic_decay_factor_ = [0.8, 0.9, 0.99]\n",
    "            elif mm_policy == \"episodic\":\n",
    "                qa_function_ = [\"episodic\"]\n",
    "                pretrain_semantic_ = [False]\n",
    "                capacity = {\n",
    "                    \"episodic\": capacity_max,\n",
    "                    \"semantic\": 0,\n",
    "                    \"short\": 1,\n",
    "                }\n",
    "            elif mm_policy == \"semantic\":\n",
    "                qa_function_ = [\"semantic\"]\n",
    "                capacity = {\n",
    "                    \"episodic\": 0,\n",
    "                    \"semantic\": capacity_max,\n",
    "                    \"short\": 1,\n",
    "                }\n",
    "                pretrain_semantic_ = [False, \"include_walls\", \"exclude_walls\"]\n",
    "                semantic_decay_factor_ = [0.8, 0.9, 0.99]\n",
    "\n",
    "            for explore_policy in [\"random\", \"avoid_walls\"]:\n",
    "                for qa_function in qa_function_:\n",
    "                    for pretrain_semantic in pretrain_semantic_:\n",
    "                        for semantic_decay_factor in semantic_decay_factor_:\n",
    "                            for seed in range(num_seeds):\n",
    "\n",
    "                                agent = HandcraftedAgent(\n",
    "                                    env_str=env_str,\n",
    "                                    env_config={**env_config, \"seed\": seed},\n",
    "                                    mm_policy=mm_policy,\n",
    "                                    qa_function=qa_function,\n",
    "                                    explore_policy=explore_policy,\n",
    "                                    num_samples_for_results=10,\n",
    "                                    capacity=capacity,\n",
    "                                    pretrain_semantic=pretrain_semantic,\n",
    "                                    semantic_decay_factor=semantic_decay_factor,\n",
    "                                    default_root_dir=os.path.join(\n",
    "                                        root_path,\n",
    "                                        f\"room_size={room_size}/mm_policy={mm_policy}/qa_function={qa_function}/explore_policy={explore_policy}/episodiccapacity={capacity['episodic']}/semanticcapacity={capacity['semantic']}/shortcapacity={capacity['short']}/pretrain_semantic={pretrain_semantic}/semantic_decay_factor={semantic_decay_factor}/\",\n",
    "                                    ),\n",
    "                                )\n",
    "                                agent.test()\n",
    "\n",
    "    def parse_hyper_params_from_path(path):\n",
    "        \"\"\"Extracts hyperparameters from the given file path.\"\"\"\n",
    "        try:\n",
    "            path_parts = path.split(\"/\")\n",
    "            return {\n",
    "                \"mm_policy\": path_parts[5].split(\"=\")[-1],\n",
    "                \"qa_function\": path_parts[6].split(\"=\")[-1],\n",
    "                \"explore_policy\": path_parts[7].split(\"=\")[-1],\n",
    "                \"episodic_capacity\": int(path_parts[8].split(\"=\")[-1]),\n",
    "                \"semantic_capacity\": int(path_parts[9].split(\"=\")[-1]),\n",
    "                \"long_capacity\": int(path_parts[8].split(\"=\")[-1])\n",
    "                + int(path_parts[9].split(\"=\")[-1]),\n",
    "                \"short_capacity\": int(path_parts[10].split(\"=\")[-1]),\n",
    "                \"pretrain_semantic\": path_parts[11].split(\"=\")[-1],\n",
    "                \"semantic_decay_factor\": float(path_parts[12].split(\"=\")[-1]),\n",
    "            }\n",
    "        except (IndexError, ValueError) as e:\n",
    "            print(f\"Error parsing hyperparameters from path {path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_results(yaml_paths):\n",
    "        \"\"\"Loads YAML data from a list of file paths.\"\"\"\n",
    "        results = []\n",
    "        for path in yaml_paths:\n",
    "            try:\n",
    "                results.append(read_yaml(path))\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading YAML file {path}: {e}\")\n",
    "        return results\n",
    "\n",
    "    results_paths = glob(\n",
    "        os.path.join(root_path, f\"room_size={room_size}/*/*/*/*/*/*/*/*/*/results.yaml\")\n",
    "    )\n",
    "\n",
    "    print(f\"Before grouping: {len(results_paths)} results\")\n",
    "\n",
    "    # Group results by hyperparameters\n",
    "    grouped_results = defaultdict(list)\n",
    "    for path in results_paths:\n",
    "        hyper_params = parse_hyper_params_from_path(path)\n",
    "        if hyper_params is not None:\n",
    "            results_data = read_yaml(path)\n",
    "            hp_tuple = tuple(sorted(hyper_params.items()))\n",
    "            grouped_results[hp_tuple].append(results_data)\n",
    "\n",
    "    print(f\"After grouping: {len(grouped_results)} results\")\n",
    "\n",
    "    # Simplify results to mean and std of test scores\n",
    "    simplified_results = []\n",
    "    for hp_tuple, results in grouped_results.items():\n",
    "        mean_scores = [result[\"test_score\"][\"mean\"] for result in results]\n",
    "\n",
    "        mean_of_means = np.mean(mean_scores)\n",
    "        std_of_means = np.std(mean_scores)\n",
    "\n",
    "        simplified_results.append(\n",
    "            {\n",
    "                \"hyper_params\": dict(hp_tuple),\n",
    "                \"results\": {\n",
    "                    \"test_mean\": mean_of_means.item(),\n",
    "                    \"test_std\": std_of_means.item(),\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "    print(f\"After simplifying: {len(simplified_results)} results\")\n",
    "\n",
    "    # filtered results\n",
    "    filtered_results = []\n",
    "    for result in simplified_results:\n",
    "        if result[\"hyper_params\"][\"episodic_capacity\"] == 0:\n",
    "            if result[\"hyper_params\"][\"mm_policy\"] == \"episodic\":\n",
    "                continue\n",
    "            if result[\"hyper_params\"][\"qa_function\"] == \"episodic\":\n",
    "                continue\n",
    "\n",
    "        if result[\"hyper_params\"][\"semantic_capacity\"] == 0:\n",
    "            if result[\"hyper_params\"][\"mm_policy\"] == \"semantic\":\n",
    "                continue\n",
    "            if result[\"hyper_params\"][\"qa_function\"] == \"semantic\":\n",
    "                continue\n",
    "\n",
    "        if result[\"hyper_params\"][\"mm_policy\"] == \"episodic\":\n",
    "            if result[\"hyper_params\"][\"qa_function\"] == \"semantic\":\n",
    "                continue\n",
    "            if result[\"hyper_params\"][\"qa_function\"] == \"random\":\n",
    "                continue\n",
    "            if result[\"hyper_params\"][\"qa_function\"] == \"episodic_semantic\":\n",
    "                continue\n",
    "            if result[\"hyper_params\"][\"semantic_capacity\"] > 0:\n",
    "                continue\n",
    "\n",
    "        if result[\"hyper_params\"][\"mm_policy\"] == \"semantic\":\n",
    "            if result[\"hyper_params\"][\"qa_function\"] == \"episodic\":\n",
    "                continue\n",
    "            if result[\"hyper_params\"][\"qa_function\"] == \"random\":\n",
    "                continue\n",
    "            if result[\"hyper_params\"][\"qa_function\"] == \"episodic_semantic\":\n",
    "                continue\n",
    "            if result[\"hyper_params\"][\"episodic_capacity\"] > 0:\n",
    "                continue\n",
    "\n",
    "        if result[\"hyper_params\"][\"mm_policy\"] == \"generalize\":\n",
    "            if result[\"hyper_params\"][\"qa_function\"] == \"episodic\":\n",
    "                continue\n",
    "            if result[\"hyper_params\"][\"qa_function\"] == \"semantic\":\n",
    "                continue\n",
    "\n",
    "        if result[\"hyper_params\"][\"mm_policy\"] == \"random\":\n",
    "            if result[\"hyper_params\"][\"qa_function\"] == \"episodic\":\n",
    "                continue\n",
    "            if result[\"hyper_params\"][\"qa_function\"] == \"semantic\":\n",
    "                continue\n",
    "\n",
    "        filtered_results.append(result)\n",
    "\n",
    "    print(f\"After filtering: {len(filtered_results)} results\")\n",
    "\n",
    "    # Create a DataFrame from the combined results\n",
    "    data = []\n",
    "    for item in filtered_results:\n",
    "        row = item[\"hyper_params\"]\n",
    "        row.update(item[\"results\"])\n",
    "        data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Define the desired column order\n",
    "    column_order = [\n",
    "        \"mm_policy\",\n",
    "        \"qa_function\",\n",
    "        \"explore_policy\",\n",
    "        \"pretrain_semantic\",\n",
    "        \"long_capacity\",\n",
    "        \"episodic_capacity\",\n",
    "        \"semantic_capacity\",\n",
    "        \"short_capacity\",\n",
    "        \"semantic_decay_factor\",\n",
    "        \"test_mean\",\n",
    "        \"test_std\",\n",
    "    ]\n",
    "\n",
    "    df = df.sort_values(by=column_order, ascending=True)\n",
    "    df = df.sort_values(by=[\"long_capacity\", \"test_mean\"], ascending=[True, False])\n",
    "\n",
    "    # Reorder the DataFrame columns\n",
    "    df = df[column_order]\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(\n",
    "        os.path.join(root_path, f\"hand-crafted-results-room_size={room_size}.csv\"),\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "    # Confirm the DataFrame is saved by printing the location\n",
    "    print(os.path.join(root_path, f\"hand-crafted-results-room_size={room_size}.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure the average observations per room, by room size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.disabled = True\n",
    "\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "for room_size in [\"xxs\", \"xs\", \"s\", \"m\", \"l\", \"xl\", \"xxl\"]:\n",
    "    num_obs = []\n",
    "\n",
    "    env_str = \"room_env:RoomEnv-v2\"\n",
    "    env_config = {\n",
    "        \"question_prob\": 1.0,\n",
    "        \"terminates_at\": 99,\n",
    "        \"randomize_observations\": \"objects\",\n",
    "        \"room_size\": room_size,\n",
    "        \"rewards\": {\"correct\": 1, \"wrong\": 0, \"partial\": 0},\n",
    "        \"make_everything_static\": False,\n",
    "        \"num_total_questions\": 1000,\n",
    "        \"question_interval\": 5,\n",
    "        \"include_walls_in_observations\": True,\n",
    "        \"deterministic_objects\": False,\n",
    "    }\n",
    "\n",
    "    env = gym.make(\"room_env:RoomEnv-v2\", room_size=room_size)\n",
    "    observations, info = env.reset()\n",
    "\n",
    "    while True:\n",
    "        observations, reward, done, truncated, info = env.step(\n",
    "            (\n",
    "                [\"random answer\"] * len(observations[\"questions\"]),\n",
    "                random.choice([\"north\", \"east\", \"south\", \"west\"]),\n",
    "            )\n",
    "        )\n",
    "        num_obs.append(len(observations[\"room\"]))\n",
    "        if done:\n",
    "            break\n",
    "    print(\n",
    "        f\"room_size={room_size}\\tnum_obs={sum(num_obs) / len(num_obs)}\\t\"\n",
    "        f\" max_obs={max(num_obs)}\\tmin_obs={min(num_obs)}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "human-memory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
